{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as sk\n",
    "import re\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    pos_train = open('data/pos_train.txt').readlines()\n",
    "    neg_train = open('data/neg_train.txt').readlines()\n",
    "    embeddings = np.load('data/embeddings.npy')\n",
    "\n",
    "    \n",
    "    #count number of word/tweet and store it for both positive set and negative set\n",
    "    word_nbr_per_tweet_pos = np.zeros(np.shape(pos_train)[0])\n",
    "    for j in range(0,np.shape(pos_train)[0]):\n",
    "        tweet = pos_train[j]\n",
    "        size = len(tweet.split())\n",
    "        word_nbr_per_tweet_pos[j] = size\n",
    "    \n",
    "    word_nbr_per_tweet_neg = np.zeros(np.shape(neg_train)[0])\n",
    "    for j in range(0,np.shape(neg_train)[0]):\n",
    "        tweet = neg_train[j]\n",
    "        size = len(tweet.split())\n",
    "        word_nbr_per_tweet_neg[j] = size\n",
    "    \n",
    "    print(\"counting ended\")\n",
    "    \n",
    "    i=0\n",
    "    pos_mask = np.zeros(np.shape(embeddings)[1]+1)\n",
    "    pos_mask[0] +=1\n",
    "    #adding 1 at start : this is target (1 is for happy emoji, 0 or -1 for sad face)\n",
    "    training_set_pos = np.zeros(((np.shape(pos_train)[0],np.shape(embeddings)[1]+1))) + pos_mask\n",
    "    training_set_neg = np.zeros(((np.shape(neg_train)[0],np.shape(embeddings)[1]+1)))\n",
    "    vocab = open('data/vocab_cut.txt')\n",
    "    #for each word, search if it is in pos_train or neg_train\n",
    "    prevWord =\"\"\n",
    "    for word_ in vocab:\n",
    "        word = word_.split(\"\\n\")[0]\n",
    "        #if charac is ecaped, a special regex is used this is why there is a boolean\n",
    "        if(re.escape(word) != word):\n",
    "            word= re.escape(word)\n",
    "        current_emb = embeddings[i]\n",
    "        for j in range(0,np.shape(pos_train)[0]):\n",
    "            #if yes, add its embeddings.\n",
    "            #if word in pos_train[j]:\n",
    "            if(prevWord != word):\n",
    "                print(word)\n",
    "                prevWord=word\n",
    "            if re.search(r\"(?:(?:(?:\\b)|^)\"+word+\"(?:(?=\\b)|$)|(?:^|(?:\\s))\"+word+\"(?:$|(?=\\s)))\",pos_train[j]):\n",
    "                count = re.findall(r\"(?:(?:(?:\\b)|^)\"+word+\"(?:(?=\\b)|$)|(?:^|(?:\\s))\"+word+\"(?:$|(?=\\s)))\",pos_train[j])\n",
    "                if(j < 20):\n",
    "                    print(word,pos_train[j],len(count))\n",
    "                training_set_pos[j,1:np.shape(embeddings)[1]+1] += (len(count)*current_emb)\n",
    "        for j in range(0,np.shape(neg_train)[0]):\n",
    "            #if word in neg_train[j]:\n",
    "            if re.search(r\"(?:(?:(?:\\b)|^)\"+word+\"(?:(?=\\b)|$)|(?:^|(?:\\s))\"+word+\"(?:$|(?=\\s)))\",neg_train[j]):\n",
    "                count = re.findall(r\"(?:(?:(?:\\b)|^)\"+word+\"(?:(?=\\b)|$)|(?:^|(?:\\s))\"+word+\"(?:$|(?=\\s)))\",neg_train[j])\n",
    "                training_set_neg[j,1:np.shape(embeddings)[1]+1] += (len(count)*current_emb)\n",
    "        i+=1\n",
    "        if(i%5000 ==0):\n",
    "            print(\"5000 done\")\n",
    "    #then divide by number of words (averaging word vector over all words of the tweet)\n",
    "    for i in range(0,np.shape(embeddings)[1]):\n",
    "        training_set_pos[:,i+1] = training_set_pos[:,i+1]/word_nbr_per_tweet_pos\n",
    "        training_set_neg[:,i+1] = training_set_neg[:,i+1]/word_nbr_per_tweet_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,)\n",
      "(40000,)\n",
      "Yet, error is 0.355225\n",
      "(40000,)\n",
      "(40000,)\n",
      "Yet, error is 0.35715\n",
      "(40000,)\n",
      "(40000,)\n",
      "Yet, error is 0.358875\n",
      "(40000,)\n",
      "(40000,)\n",
      "Yet, error is 0.355825\n",
      "(40000,)\n",
      "(40000,)\n",
      "Yet, error is 0.357675\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "from scipy.sparse import *\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as sk\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "def construct_vectors(data,set_to_fill,vocab,embeddings):\n",
    "    list_auxiliarry_pos = [\"must\",\"need\",\"should\",\"may\",\"might\",\"can\",\"could\",\"shall\",\"would\",\"will\"]\n",
    "    list_auxiliarry_neg = [\"won't\",\"shouldn't\",\"not\",\"can't\",\"couldn't\",\"wouldn't\"]\n",
    "    counter = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "    \n",
    "    \n",
    "    for j in range(0,np.shape(data)[0]):\n",
    "        num_punctu = counter(data[j],string.punctuation)\n",
    "        list_word = data[j].split()\n",
    "        average = 0\n",
    "        num3point = 0\n",
    "        num_aux_pos =0\n",
    "        num_aux_neg =0\n",
    "        for i in list_word:\n",
    "            average+=len(i)\n",
    "            if(i==\"...\"):\n",
    "                num3point+=1\n",
    "            if(i in list_auxiliarry_pos):\n",
    "                num_aux_pos+=1\n",
    "            if(i in list_auxiliarry_neg):\n",
    "                num_aux_neg+=1\n",
    "            idx = vocab.get(i,-1)\n",
    "            if(idx>=0):\n",
    "                set_to_fill[j,1:np.shape(embeddings)[1]+1] += embeddings[idx]\n",
    "        set_to_fill[j,1:np.shape(embeddings)[1]+1] = set_to_fill[j,1:np.shape(embeddings)[1]+1]/len(list_word)\n",
    "        set_to_fill[j,np.shape(embeddings)[1]+1] = len(list_word) #add the # word\n",
    "        set_to_fill[j,np.shape(embeddings)[1]+2] = num_punctu #add the # punctuation\n",
    "        set_to_fill[j,np.shape(embeddings)[1]+3] = average/len(list_word) #add length of word in average\n",
    "        set_to_fill[j,np.shape(embeddings)[1]+4] = num_aux_pos #word in a list of auxilarry\n",
    "        set_to_fill[j,np.shape(embeddings)[1]+5] = num_aux_neg #word in a list of negative aux\n",
    "        set_to_fill[j,np.shape(embeddings)[1]+6] = num3point #number of ...\n",
    "    return set_to_fill\n",
    "def construct_features():\n",
    "    '''\n",
    "    construct a feature representation of each training tweet \n",
    "    (by averaging the word vectors over all words of the tweet).\n",
    "    '''\n",
    "    #Load the training tweets and the built GloVe word embeddings\n",
    "    additional_features = 6\n",
    "\n",
    "    pos_train = open('data/pos_train.txt').readlines()\n",
    "    neg_train = open('data/neg_train.txt').readlines()\n",
    "    embeddings = np.load('data/embeddings.npy')\n",
    "    with open('data/vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    pos_mask = np.zeros(np.shape(embeddings)[1]+1+additional_features)\n",
    "    pos_mask[0] +=1\n",
    "    #adding 1 at start : this is target (1 is for happy emoji, 0 or -1 for sad face)\n",
    "    #will add 3 features , number of word , average length of words, and #punctuation\n",
    "    training_set_pos = np.zeros(((np.shape(pos_train)[0],np.shape(embeddings)[1]+1+additional_features))) + pos_mask\n",
    "    training_set_neg = np.zeros(((np.shape(neg_train)[0],np.shape(embeddings)[1]+1+additional_features)))\n",
    "    #for each word, search if it is in pos_train or neg_train\n",
    "    \n",
    "    training_set_pos = construct_vectors(pos_train,training_set_pos,vocab,embeddings) #look at method above\n",
    "    training_set_neg = construct_vectors(neg_train,training_set_neg,vocab,embeddings)\n",
    "    np.save('data/trainingset_pos', training_set_pos)\n",
    "    np.save('data/trainingset_neg', training_set_neg)\n",
    "    \n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})\n",
    "\n",
    "def predict_labels(flag=\".npy\"):\n",
    "    #Load the training set\n",
    "    path_neg = str(\"data/trainingset_neg\"+flag)\n",
    "    path_pos = str(\"data/trainingset_pos\"+flag)\n",
    "    ts_neg = np.load(path_neg)\n",
    "    ts_pos = np.load(path_pos)    \n",
    "    #Train a Linear Classifier: Train a linear classifier (e.g. logistic regression or SVM) on your constructed \n",
    "    #features, using the scikit learn library, or your own code from the earlier labs. Recall that the labels \n",
    "    #indicate if a tweet used to contain a :) or :( smiley.\n",
    "    training_set = np.concatenate((ts_neg,ts_pos))\n",
    "    y = training_set[:,0]\n",
    "    X = training_set[:,1:np.shape(training_set)[1]]\n",
    "    X = build_poly(X,2)\n",
    "    #Now we load and predict the data\n",
    "    data = np.genfromtxt('data/test_data.txt', delimiter=\"\\n\",dtype=str)    \n",
    "    idx = np.zeros(np.shape(data)[0])\n",
    "    tweets = [\"\" for a in range(0,np.shape(data)[0])]\n",
    "    for i in range(0,np.shape(data)[0]):\n",
    "        spliter = data[i].split(\",\")\n",
    "        idx[i] = spliter[0]\n",
    "        tweet = spliter[1]\n",
    "        for j in range(2,np.shape(spliter)[0]):\n",
    "            tweet = tweet+\",\"+spliter[j]\n",
    "        tweets[i] = tweet\n",
    "    \n",
    "    #Construct the logistic regressor\n",
    "    LR = sk.LogisticRegressionCV()\n",
    "    #LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, \n",
    "    #class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, \n",
    "    #warm_start=False, n_jobs=1)[source]¶\n",
    "    #http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "    #train the logistic regressor\n",
    "    kf = ms.KFold(n_splits=5,shuffle=True)\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        train_set = X[train_idx]\n",
    "        test_set = X[test_idx]\n",
    "        train_target = y[train_idx]\n",
    "        test_target = y[test_idx]    \n",
    "        LR.fit(train_set,train_target)\n",
    "        predictions_temp = LR.predict(test_set)\n",
    "        print(predictions_temp.shape)\n",
    "        print(test_target.shape)        \n",
    "        error = np.sum(np.power(predictions_temp-test_target,2))/np.shape(predictions_temp)[0]\n",
    "        print(\"Yet, error is\",error)\n",
    "    LR.fit(X,y)\n",
    "    \n",
    "    #And now, predict the results\n",
    "    topredict = construct_features_for_test_set(tweets)\n",
    "    topredict = build_poly(topredict,2)\n",
    "    predictions = LR.predict(topredict)\n",
    "    #Construct the submission\n",
    "    predictions = predictions*2-1\n",
    "    create_csv_submission(idx,predictions,\"submission.csv\")\n",
    "\n",
    "def construct_features_for_test_set(test_set_tweet):\n",
    "    embeddings = np.load('data/embeddings.npy')\n",
    "    with open('data/vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    additional_features = 6\n",
    "    list_auxiliarry_pos = [\"must\",\"need\",\"should\",\"may\",\"might\",\"can\",\"could\",\"shall\",\"would\",\"will\"]\n",
    "    list_auxiliarry_neg = [\"won't\",\"shouldn't\",\"not\",\"can't\",\"couldn't\",\"wouldn't\"]\n",
    "    test_set = np.zeros((np.shape(test_set_tweet)[0],np.shape(embeddings)[1]+additional_features))\n",
    "    #for each word, search if it is in a tweet\n",
    "    for j in range(0,np.shape(test_set)[0]):\n",
    "        list_word = test_set_tweet[j].split()\n",
    "        divider = 0\n",
    "        average = 0\n",
    "        num3point = 0\n",
    "        num_aux_pos = 0\n",
    "        num_aux_neg = 0\n",
    "        counter = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "        num_punctu = counter(test_set_tweet[j],string.punctuation)\n",
    "        for i in list_word:\n",
    "            idx = vocab.get(i,-1)\n",
    "            average+=len(i)\n",
    "            if(i==\"...\"):\n",
    "                num3point+=1\n",
    "            if(i in list_auxiliarry_pos):\n",
    "                num_aux_pos+=1\n",
    "            if(i in list_auxiliarry_neg):\n",
    "                num_aux_neg+=1\n",
    "            if(idx>=0):\n",
    "                divider+=1\n",
    "                test_set[j,:np.shape(embeddings)[1]] += embeddings[idx]\n",
    "        if(divider >0):\n",
    "            test_set[j,:np.shape(embeddings)[1]] = test_set[j,:np.shape(embeddings)[1]]/divider\n",
    "        test_set[j,np.shape(embeddings)[1]] = len(list_word) #add the # word\n",
    "        test_set[j,np.shape(embeddings)[1]+1] = num_punctu #add the # punctuation\n",
    "        if(len(list_word) >0):\n",
    "            test_set[j,np.shape(embeddings)[1]+2] = average/len(list_word)#add length of word in average\n",
    "        else : \n",
    "            test_set[j,np.shape(embeddings)[1]+2] = 0\n",
    "        test_set[j,np.shape(embeddings)[1]+3] = num_aux_pos\n",
    "        test_set[j,np.shape(embeddings)[1]+4] = num_aux_neg\n",
    "        test_set[j,np.shape(embeddings)[1]+5] = num3point\n",
    "    #then divide by number of words (averaging word vector over all words of the tweet)\n",
    "    return test_set\n",
    "construct_features()\n",
    "predict_labels()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test = np.zeros((3,4))\n",
    "print(test[:,1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = [\"...\",\"mustn't\"]\n",
    "if(\"mustn't\" in test):\n",
    "    print(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
