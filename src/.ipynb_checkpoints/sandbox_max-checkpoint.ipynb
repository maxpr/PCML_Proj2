{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as sk\n",
    "import re\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    pos_train = open('data/pos_train.txt').readlines()\n",
    "    neg_train = open('data/neg_train.txt').readlines()\n",
    "    embeddings = np.load('data/embeddings.npy')\n",
    "\n",
    "    \n",
    "    #count number of word/tweet and store it for both positive set and negative set\n",
    "    word_nbr_per_tweet_pos = np.zeros(np.shape(pos_train)[0])\n",
    "    for j in range(0,np.shape(pos_train)[0]):\n",
    "        tweet = pos_train[j]\n",
    "        size = len(tweet.split())\n",
    "        word_nbr_per_tweet_pos[j] = size\n",
    "    \n",
    "    word_nbr_per_tweet_neg = np.zeros(np.shape(neg_train)[0])\n",
    "    for j in range(0,np.shape(neg_train)[0]):\n",
    "        tweet = neg_train[j]\n",
    "        size = len(tweet.split())\n",
    "        word_nbr_per_tweet_neg[j] = size\n",
    "    \n",
    "    print(\"counting ended\")\n",
    "    \n",
    "    i=0\n",
    "    pos_mask = np.zeros(np.shape(embeddings)[1]+1)\n",
    "    pos_mask[0] +=1\n",
    "    #adding 1 at start : this is target (1 is for happy emoji, 0 or -1 for sad face)\n",
    "    training_set_pos = np.zeros(((np.shape(pos_train)[0],np.shape(embeddings)[1]+1))) + pos_mask\n",
    "    training_set_neg = np.zeros(((np.shape(neg_train)[0],np.shape(embeddings)[1]+1)))\n",
    "    vocab = open('data/vocab_cut.txt')\n",
    "    #for each word, search if it is in pos_train or neg_train\n",
    "    prevWord =\"\"\n",
    "    for word_ in vocab:\n",
    "        word = word_.split(\"\\n\")[0]\n",
    "        #if charac is ecaped, a special regex is used this is why there is a boolean\n",
    "        if(re.escape(word) != word):\n",
    "            word= re.escape(word)\n",
    "        current_emb = embeddings[i]\n",
    "        for j in range(0,np.shape(pos_train)[0]):\n",
    "            #if yes, add its embeddings.\n",
    "            #if word in pos_train[j]:\n",
    "            if(prevWord != word):\n",
    "                print(word)\n",
    "                prevWord=word\n",
    "            if re.search(r\"(?:(?:(?:\\b)|^)\"+word+\"(?:(?=\\b)|$)|(?:^|(?:\\s))\"+word+\"(?:$|(?=\\s)))\",pos_train[j]):\n",
    "                count = re.findall(r\"(?:(?:(?:\\b)|^)\"+word+\"(?:(?=\\b)|$)|(?:^|(?:\\s))\"+word+\"(?:$|(?=\\s)))\",pos_train[j])\n",
    "                if(j < 20):\n",
    "                    print(word,pos_train[j],len(count))\n",
    "                training_set_pos[j,1:np.shape(embeddings)[1]+1] += (len(count)*current_emb)\n",
    "        for j in range(0,np.shape(neg_train)[0]):\n",
    "            #if word in neg_train[j]:\n",
    "            if re.search(r\"(?:(?:(?:\\b)|^)\"+word+\"(?:(?=\\b)|$)|(?:^|(?:\\s))\"+word+\"(?:$|(?=\\s)))\",neg_train[j]):\n",
    "                count = re.findall(r\"(?:(?:(?:\\b)|^)\"+word+\"(?:(?=\\b)|$)|(?:^|(?:\\s))\"+word+\"(?:$|(?=\\s)))\",neg_train[j])\n",
    "                training_set_neg[j,1:np.shape(embeddings)[1]+1] += (len(count)*current_emb)\n",
    "        i+=1\n",
    "        if(i%5000 ==0):\n",
    "            print(\"5000 done\")\n",
    "    #then divide by number of words (averaging word vector over all words of the tweet)\n",
    "    for i in range(0,np.shape(embeddings)[1]):\n",
    "        training_set_pos[:,i+1] = training_set_pos[:,i+1]/word_nbr_per_tweet_pos\n",
    "        training_set_neg[:,i+1] = training_set_neg[:,i+1]/word_nbr_per_tweet_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings = np.load('data/embeddings.npy')\n",
    "with open('data/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "    vocab_size = len(vocab)\n",
    "print(vocab.get(\"<user>\",-1))\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(40000,)\n",
      "(40000,)\n",
      "Yet, error is 0.361675\n",
      "(40000,)\n",
      "(40000,)\n",
      "Yet, error is 0.369275\n",
      "(40000,)\n",
      "(40000,)\n",
      "Yet, error is 0.358175\n",
      "(40000,)\n",
      "(40000,)\n",
      "Yet, error is 0.363425\n",
      "(40000,)\n",
      "(40000,)\n",
      "Yet, error is 0.42555\n",
      "2\n",
      "(40000,)\n",
      "(40000,)\n",
      "Yet, error is 0.4712\n",
      "(40000,)\n",
      "(40000,)\n",
      "Yet, error is 0.386575\n",
      "(40000,)\n",
      "(40000,)\n",
      "Yet, error is 0.43735\n",
      "(40000,)\n",
      "(40000,)\n",
      "Yet, error is 0.433475\n",
      "(40000,)\n",
      "(40000,)\n",
      "Yet, error is 0.49135\n",
      "3\n",
      "(40000,)\n",
      "(40000,)\n",
      "Yet, error is 0.54125\n",
      "(40000,)\n",
      "(40000,)\n",
      "Yet, error is 0.577175\n",
      "(40000,)\n",
      "(40000,)\n",
      "Yet, error is 0.4723\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-105cb1f52050>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[0mconstruct_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m \u001b[0mpredict_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-105cb1f52050>\u001b[0m in \u001b[0;36mpredict_labels\u001b[1;34m(flag)\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[1;31m#LR.fit(train_set,train_target)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[1;31m#predictions_temp = LR.predict(test_set)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m             \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m             \u001b[0mpredictions_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions_temp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Max-Pc\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m             self.loss, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"crammer_singer\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Max-Pc\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m         epsilon, sample_weight)\n\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m     \u001b[1;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "from scipy.sparse import *\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as sk\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "def construct_vectors(data,set_to_fill,vocab,embeddings):\n",
    "    list_auxiliarry_pos = [\"must\",\"need\",\"should\",\"may\",\"might\",\"can\",\"could\",\"shall\",\"would\",\"will\"]\n",
    "    list_auxiliarry_neg = [\"won't\",\"shouldn't\",\"not\",\"can't\",\"couldn't\",\"wouldn't\"]\n",
    "    counter = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "    \n",
    "    \n",
    "    for j in range(0,np.shape(data)[0]):\n",
    "        num_punctu = counter(data[j],string.punctuation)\n",
    "        list_word = data[j].split()\n",
    "        average = 0\n",
    "        num3point = 0\n",
    "        num_aux_pos =0\n",
    "        num_aux_neg =0\n",
    "        for i in list_word:\n",
    "            average+=len(i)\n",
    "            if(i==\"...\"):\n",
    "                num3point+=1\n",
    "            if(i in list_auxiliarry_pos):\n",
    "                num_aux_pos+=1\n",
    "            if(i in list_auxiliarry_neg):\n",
    "                num_aux_neg+=1\n",
    "            idx = vocab.get(i,-1)\n",
    "            if(idx>=0):\n",
    "                set_to_fill[j,1:np.shape(embeddings)[1]+1] += embeddings[idx]\n",
    "        set_to_fill[j,1:np.shape(embeddings)[1]+1] = set_to_fill[j,1:np.shape(embeddings)[1]+1]/len(list_word)\n",
    "        set_to_fill[j,np.shape(embeddings)[1]+1] = len(list_word) #add the # word\n",
    "        set_to_fill[j,np.shape(embeddings)[1]+2] = num_punctu #add the # punctuation\n",
    "        set_to_fill[j,np.shape(embeddings)[1]+3] = average/len(list_word) #add length of word in average\n",
    "        set_to_fill[j,np.shape(embeddings)[1]+4] = num_aux_pos #word in a list of auxilarry\n",
    "        set_to_fill[j,np.shape(embeddings)[1]+5] = num_aux_neg #word in a list of negative aux\n",
    "        set_to_fill[j,np.shape(embeddings)[1]+6] = num3point #number of ...\n",
    "    return set_to_fill\n",
    "def construct_features():\n",
    "    '''\n",
    "    construct a feature representation of each training tweet \n",
    "    (by averaging the word vectors over all words of the tweet).\n",
    "    '''\n",
    "    #Load the training tweets and the built GloVe word embeddings\n",
    "    additional_features = 6\n",
    "\n",
    "    pos_train = open('data/pos_train.txt').readlines()\n",
    "    neg_train = open('data/neg_train.txt').readlines()\n",
    "    embeddings = np.load('data/embeddings.npy')\n",
    "    with open('data/vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    pos_mask = np.zeros(np.shape(embeddings)[1]+1+additional_features)\n",
    "    pos_mask[0] +=1\n",
    "    #adding 1 at start : this is target (1 is for happy emoji, 0 or -1 for sad face)\n",
    "    #will add 3 features , number of word , average length of words, and #punctuation\n",
    "    training_set_pos = np.zeros(((np.shape(pos_train)[0],np.shape(embeddings)[1]+1+additional_features))) + pos_mask\n",
    "    training_set_neg = np.zeros(((np.shape(neg_train)[0],np.shape(embeddings)[1]+1+additional_features)))\n",
    "    #for each word, search if it is in pos_train or neg_train\n",
    "    \n",
    "    training_set_pos = construct_vectors(pos_train,training_set_pos,vocab,embeddings) #look at method above\n",
    "    training_set_neg = construct_vectors(neg_train,training_set_neg,vocab,embeddings)\n",
    "    np.save('data/trainingset_pos', training_set_pos)\n",
    "    np.save('data/trainingset_neg', training_set_neg)\n",
    "    \n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})\n",
    "\n",
    "def predict_labels(flag=\".npy\"):\n",
    "    #Load the training set\n",
    "    path_neg = str(\"data/trainingset_neg\"+flag)\n",
    "    path_pos = str(\"data/trainingset_pos\"+flag)\n",
    "    ts_neg = np.load(path_neg)\n",
    "    ts_pos = np.load(path_pos)    \n",
    "    #Train a Linear Classifier: Train a linear classifier (e.g. logistic regression or SVM) on your constructed \n",
    "    #features, using the scikit learn library, or your own code from the earlier labs. Recall that the labels \n",
    "    #indicate if a tweet used to contain a :) or :( smiley.\n",
    "    training_set = np.concatenate((ts_neg,ts_pos))\n",
    "    y = training_set[:,0]\n",
    "    X = training_set[:,1:np.shape(training_set)[1]]\n",
    "    X = build_poly(X,2)\n",
    "    #Now we load and predict the data\n",
    "    data = np.genfromtxt('data/test_data.txt', delimiter=\"\\n\",dtype=str)    \n",
    "    idx = np.zeros(np.shape(data)[0])\n",
    "    tweets = [\"\" for a in range(0,np.shape(data)[0])]\n",
    "    for i in range(0,np.shape(data)[0]):\n",
    "        spliter = data[i].split(\",\")\n",
    "        idx[i] = spliter[0]\n",
    "        tweet = spliter[1]\n",
    "        for j in range(2,np.shape(spliter)[0]):\n",
    "            tweet = tweet+\",\"+spliter[j]\n",
    "        tweets[i] = tweet\n",
    "    \n",
    "    #Construct the logistic regressor\n",
    "    LR = sk.LogisticRegressionCV()\n",
    "    #LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, \n",
    "    #class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, \n",
    "    #warm_start=False, n_jobs=1)[source]Â¶\n",
    "    #http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "    #train the logistic regressor\n",
    "    kf = ms.KFold(n_splits=5,shuffle=True)\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        train_set = X[train_idx]\n",
    "        test_set = X[test_idx]\n",
    "        train_target = y[train_idx]\n",
    "        test_target = y[test_idx]    \n",
    "        LR.fit(train_set,train_target)\n",
    "        predictions_temp = LR.predict(test_set)\n",
    "        print(predictions_temp.shape)\n",
    "        print(test_target.shape)        \n",
    "        error = np.sum(np.power(predictions_temp-test_target,2))/np.shape(predictions_temp)[0]\n",
    "        print(\"Yet, error is\",error)\n",
    "    LR.fit(X,y)\n",
    "    \n",
    "    #And now, predict the results\n",
    "    topredict = construct_features_for_test_set(tweets)\n",
    "    topredict = build_poly(topredict,2)\n",
    "    predictions = LR.predict(topredict)\n",
    "    #Construct the submission\n",
    "    predictions = predictions*2-1\n",
    "    create_csv_submission(idx,predictions,\"submission.csv\")\n",
    "\n",
    "def construct_features_for_test_set(test_set_tweet):\n",
    "    embeddings = np.load('data/embeddings.npy')\n",
    "    with open('data/vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    additional_features = 6\n",
    "    list_auxiliarry_pos = [\"must\",\"need\",\"should\",\"may\",\"might\",\"can\",\"could\",\"shall\",\"would\",\"will\"]\n",
    "    list_auxiliarry_neg = [\"won't\",\"shouldn't\",\"not\",\"can't\",\"couldn't\",\"wouldn't\"]\n",
    "    test_set = np.zeros((np.shape(test_set_tweet)[0],np.shape(embeddings)[1]+additional_features))\n",
    "    #for each word, search if it is in a tweet\n",
    "    for j in range(0,np.shape(test_set)[0]):\n",
    "        list_word = test_set_tweet[j].split()\n",
    "        divider = 0\n",
    "        average = 0\n",
    "        num3point = 0\n",
    "        num_aux_pos = 0\n",
    "        num_aux_neg = 0\n",
    "        counter = lambda l1, l2: len(list(filter(lambda c: c in l2, l1)))\n",
    "        num_punctu = counter(test_set_tweet[j],string.punctuation)\n",
    "        for i in list_word:\n",
    "            idx = vocab.get(i,-1)\n",
    "            average+=len(i)\n",
    "            if(i==\"...\"):\n",
    "                num3point+=1\n",
    "            if(i in list_auxiliarry_pos):\n",
    "                num_aux_pos+=1\n",
    "            if(i in list_auxiliarry_neg):\n",
    "                num_aux_neg+=1\n",
    "            if(idx>=0):\n",
    "                divider+=1\n",
    "                test_set[j,:np.shape(embeddings)[1]] += embeddings[idx]\n",
    "        if(divider >0):\n",
    "            test_set[j,:np.shape(embeddings)[1]] = test_set[j,:np.shape(embeddings)[1]]/divider\n",
    "        test_set[j,np.shape(embeddings)[1]] = len(list_word) #add the # word\n",
    "        test_set[j,np.shape(embeddings)[1]+1] = num_punctu #add the # punctuation\n",
    "        if(len(list_word) >0):\n",
    "            test_set[j,np.shape(embeddings)[1]+2] = average/len(list_word)#add length of word in average\n",
    "        else : \n",
    "            test_set[j,np.shape(embeddings)[1]+2] = 0\n",
    "        test_set[j,np.shape(embeddings)[1]+3] = num_aux_pos\n",
    "        test_set[j,np.shape(embeddings)[1]+4] = num_aux_neg\n",
    "        test_set[j,np.shape(embeddings)[1]+5] = num3point\n",
    "    #then divide by number of words (averaging word vector over all words of the tweet)\n",
    "    return test_set\n",
    "construct_features()\n",
    "predict_labels()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test = np.zeros((3,4))\n",
    "print(test[:,1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = [\"...\",\"mustn't\"]\n",
    "if(\"mustn't\" in test):\n",
    "    print(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
