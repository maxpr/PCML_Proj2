{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 4,
>>>>>>> eb8cf5f799c31644c355cb260a9b32b2ca795f51
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as sk\n",
    "import re\n",
    "from glove_routines import *\n",
    "from text_classifier import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 197,
=======
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_emb5_eta0.0001_alpha0.1_nmax50_epochs1\n",
      "loading cooccurrence matrix\n",
      "6496907 nonzero entries\n",
      "initializing parameters : nmax = 50 ,cooc.max() = 207302 , embedding_dim = 5 , eta = 0.0001 , alpha = 0.1 , epochs = 1 .\n",
      "initializing embeddings\n",
      "initializing cost\n",
      "Running now\n",
      "epoch 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-03a41ef6a0d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_emb\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_eta\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_alpha\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_nmax\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_epochs\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglove_SGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m                     \u001b[1;32massert\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andre\\Documents\\EPFL\\Boulot\\PCML\\project2\\src\\glove_routines.py\u001b[0m in \u001b[0;36mglove_SGD\u001b[1;34m(embedding_dim, eta, alpha, nmax, epochs, track_losses, flags)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mnewY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscale\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_losses\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlogn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnewX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnewY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m             \u001b[0mxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewY\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andre\\Documents\\EPFL\\Boulot\\PCML\\project2\\src\\glove_routines.py\u001b[0m in \u001b[0;36mupdate_cost\u001b[1;34m(cost, fn, x, w, z, newW, newZ)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mupdate_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m*\u001b[0m \u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m*\u001b[0m \u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#def glove_SGD(embedding_dim = 20, eta = 0.001, alpha = 3 / 4, nmax = 100, \n",
    "#epochs = 10, track_losses = -1, flags=\"\"):\n",
    "embedding_dim = [5, 30, 50, 100, 200]\n",
    "eta = [0.0001, 0.001, 0.01]\n",
    "alpha = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "nmax = [50, 75, 100, 150, 200]\n",
    "epochs = [1, 4, 6]\n",
    "costs = []\n",
    "for emb in embedding_dim:\n",
    "    for e in eta:\n",
    "        for a in alpha:\n",
    "            for nm in nmax:\n",
    "                for ep in epochs:\n",
    "                    flag = str(\"_emb\"+str(emb)+\"_eta\"+str(e)+\"_alpha\"+str(a)+\"_nmax\"+str(nm)+\"_epochs\"+str(ep))\n",
    "                    print(\"doing :\",flag)\n",
    "                    cost = glove_SGD(embedding_dim=emb, eta=e, alpha=a, nmax=nm, epochs=ep,flags=flag, track_losses=0)\n",
    "                    print(\"DONE :\",flag)\n",
    "                    costs.append(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32484536,)\n",
      "(32484536,)\n",
      "(32484536,)\n",
      "(32484536,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAETCAYAAADTbHYBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1feb77f24a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_losses(flags=\"\"):\n",
    "    losses = np.load(str('metadata/embeddings_cost'+flags+\".npy\"))\n",
    "    print(np.shape(losses))\n",
    "    plt.figure()\n",
    "    plt.title(flags)\n",
    "    plt.plot(losses)\n",
    "plot_losses(\"_025alpha\")\n",
    "plot_losses(\"_60emb\")\n",
    "plot_losses(\"_basic\")\n",
    "plot_losses(\"_70nmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_digits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-ea451f8ba90e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m \u001b[0mdigits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_digits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdigits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_digits' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> eb8cf5f799c31644c355cb260a9b32b2ca795f51
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\:\\'\\)\n",
      "[' ?']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word = \"lol wutwut (8 lol ? <user> lol , quoi ahah lol wutwut lol lol\"\n",
    "smiley = \":')\"\n",
    "print(re.escape(smiley))\n",
    "test = word.split()\n",
    "listesc = \".^$*+?{}[]\\|()\" #list of all escaped charactes\n",
    "if('\\(8' in listesc):\n",
    "    print(\"loel\")\n",
    "pd = \"\\?\" \n",
    "if re.search(r\"(?:(?:(?:\\b)|^)\"+pd+\"(?:(?=\\b)|$)|(?:^|(?:\\s))\"+pd+\"(?:$|(?=\\s)))\",word):\n",
    "    count = re.findall(r\"(?:(?:(?:\\b)|^)\"+pd+\"(?:(?=\\b)|$)|(?:^|(?:\\s))\"+pd+\"(?:$|(?=\\s)))\",word)\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting ended\n",
      "\\<user\\>\n",
      "\\<user\\> <user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\n",
      " 1\n",
      "\\<user\\> \" <user> just put casper in a box ! \" looved the battle ! #crakkbitch\n",
      " 1\n",
      "\\<user\\> <user> <user> thanks sir > > don't trip lil mama ... just keep doin ya thang !\n",
      " 2\n",
      "\\<user\\> <user> yay ! ! #lifecompleted . tweet / facebook me to let me know please\n",
      " 1\n",
      "\\<user\\> <user> #1dnextalbumtitle : feel for you / rollercoaster of life . song cocept : life , #yolo , becoming famous ? <3 14 #followmeplz ! <3 x15\n",
      " 1\n",
      "\\<user\\> workin hard or hardly workin rt <user> at hardee's with my future coworker <user>\n",
      " 2\n",
      "\\<user\\> <user> i saw . i'll be replying in a bit .\n",
      " 1\n",
      "\\<user\\> <user> anddd to cheer #nationals2013 ?\n",
      " 1\n",
      "\\<user\\> <user> agreed ! 12 more days left tho\n",
      " 1\n",
      "\\<user\\> like dammm <user> lexis u got a lot to say when ur on twitter lol\n",
      " 1\n",
      "\\<user\\> <user> at home affairs shall do it later\n",
      " 1\n",
      "\\<user\\> <user> a lot of parts of asia . especially rats that live in the country and live on grains . supposed to be quite tasty .\n",
      " 1\n",
      "\\!\n",
      "\\! \" <user> just put casper in a box ! \" looved the battle ! #crakkbitch\n",
      " 2\n",
      "\\! <user> <user> thanks sir > > don't trip lil mama ... just keep doin ya thang !\n",
      " 1\n",
      "\\! visiting my brother tmr is the bestest birthday gift eveerrr ! ! !\n",
      " 3\n",
      "\\! <user> yay ! ! #lifecompleted . tweet / facebook me to let me know please\n",
      " 2\n",
      "\\! <user> #1dnextalbumtitle : feel for you / rollercoaster of life . song cocept : life , #yolo , becoming famous ? <3 14 #followmeplz ! <3 x15\n",
      " 1\n",
      "\\! we send an invitation to shop on-line ! here you will find everything you need - without leaving home ... <url>\n",
      " 1\n",
      "\\! <user> agreed ! 12 more days left tho\n",
      " 1\n",
      "\\! grateful today for a dream fulfilled ! ! my heart is so full - first 3 completed tracks have arrived back from new york ! #yeslord !\n",
      " 4\n",
      "i\n",
      "i <user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\n",
      " 2\n",
      "i because your logic is so dumb , i won't even crop out your name or your photo . tsk . <url>\n",
      " 1\n",
      "i <user> i saw . i'll be replying in a bit .\n",
      " 1\n",
      "i this is were i belong\n",
      " 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-200-befd65d33838>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mtraining_set_pos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcurrent_emb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m#if word in neg_train[j]:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    pos_train = open('data/pos_train.txt').readlines()\n",
    "    neg_train = open('data/neg_train.txt').readlines()\n",
    "    embeddings = np.load('data/embeddings.npy')\n",
    "\n",
    "    \n",
    "    #count number of word/tweet and store it for both positive set and negative set\n",
    "    word_nbr_per_tweet_pos = np.zeros(np.shape(pos_train)[0])\n",
    "    for j in range(0,np.shape(pos_train)[0]):\n",
    "        tweet = pos_train[j]\n",
    "        size = len(tweet.split())\n",
    "        word_nbr_per_tweet_pos[j] = size\n",
    "    \n",
    "    word_nbr_per_tweet_neg = np.zeros(np.shape(neg_train)[0])\n",
    "    for j in range(0,np.shape(neg_train)[0]):\n",
    "        tweet = neg_train[j]\n",
    "        size = len(tweet.split())\n",
    "        word_nbr_per_tweet_neg[j] = size\n",
    "    \n",
    "    print(\"counting ended\")\n",
    "    \n",
    "    i=0\n",
    "    pos_mask = np.zeros(np.shape(embeddings)[1]+1)\n",
    "    pos_mask[0] +=1\n",
    "    #adding 1 at start : this is target (1 is for happy emoji, 0 or -1 for sad face)\n",
    "    training_set_pos = np.zeros(((np.shape(pos_train)[0],np.shape(embeddings)[1]+1))) + pos_mask\n",
    "    training_set_neg = np.zeros(((np.shape(neg_train)[0],np.shape(embeddings)[1]+1)))\n",
    "    vocab = open('data/vocab_cut.txt')\n",
    "    #for each word, search if it is in pos_train or neg_train\n",
    "    prevWord =\"\"\n",
    "    for word_ in vocab:\n",
    "        word = word_.split(\"\\n\")[0]\n",
    "        #if charac is ecaped, a special regex is used this is why there is a boolean\n",
    "        if(re.escape(word) != word):\n",
    "            word= re.escape(word)\n",
    "        current_emb = embeddings[i]\n",
    "        for j in range(0,np.shape(pos_train)[0]):\n",
    "            #if yes, add its embeddings.\n",
    "            #if word in pos_train[j]:\n",
    "            if(prevWord != word):\n",
    "                print(word)\n",
    "                prevWord=word\n",
    "            if re.search(r\"(?:(?:(?:\\b)|^)\"+word+\"(?:(?=\\b)|$)|(?:^|(?:\\s))\"+word+\"(?:$|(?=\\s)))\",pos_train[j]):\n",
    "                count = re.findall(r\"(?:(?:(?:\\b)|^)\"+word+\"(?:(?=\\b)|$)|(?:^|(?:\\s))\"+word+\"(?:$|(?=\\s)))\",pos_train[j])\n",
    "                if(j < 20):\n",
    "                    print(word,pos_train[j],len(count))\n",
    "                training_set_pos[j,1:np.shape(embeddings)[1]+1] += (len(count)*current_emb)\n",
    "        for j in range(0,np.shape(neg_train)[0]):\n",
    "            #if word in neg_train[j]:\n",
    "            if re.search(r\"(?:(?:(?:\\b)|^)\"+word+\"(?:(?=\\b)|$)|(?:^|(?:\\s))\"+word+\"(?:$|(?=\\s)))\",neg_train[j]):\n",
    "                count = re.findall(r\"(?:(?:(?:\\b)|^)\"+word+\"(?:(?=\\b)|$)|(?:^|(?:\\s))\"+word+\"(?:$|(?=\\s)))\",neg_train[j])\n",
    "                training_set_neg[j,1:np.shape(embeddings)[1]+1] += (len(count)*current_emb)\n",
    "        i+=1\n",
    "        if(i%5000 ==0):\n",
    "            print(\"5000 done\")\n",
    "    #then divide by number of words (averaging word vector over all words of the tweet)\n",
    "    for i in range(0,np.shape(embeddings)[1]):\n",
    "        training_set_pos[:,i+1] = training_set_pos[:,i+1]/word_nbr_per_tweet_pos\n",
    "        training_set_neg[:,i+1] = training_set_neg[:,i+1]/word_nbr_per_tweet_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from scipy.sparse import *\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as sk\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "\n",
    "def construct_features():\n",
    "    '''\n",
    "    construct a feature representation of each training tweet \n",
    "    (by averaging the word vectors over all words of the tweet).\n",
    "    '''\n",
    "    #Load the training tweets and the built GloVe word embeddings.\n",
    "    pos_train = open('data/pos_train.txt').readlines()\n",
    "    neg_train = open('data/neg_train.txt').readlines()\n",
    "    embeddings = np.load('data/embeddings.npy')\n",
    "    with open('data/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "    \n",
    "    #count number of word/tweet and store it for both positive set and negative set\n",
    "    word_nbr_per_tweet_pos = np.zeros(np.shape(pos_train)[0])\n",
    "    for j in range(0,np.shape(pos_train)[0]):\n",
    "        tweet = pos_train[j]\n",
    "        size = len(tweet.split())\n",
    "        word_nbr_per_tweet_pos[j] = size\n",
    "    \n",
    "    word_nbr_per_tweet_neg = np.zeros(np.shape(neg_train)[0])\n",
    "    for j in range(0,np.shape(neg_train)[0]):\n",
    "        tweet = neg_train[j]\n",
    "        size = len(tweet.split())\n",
    "        word_nbr_per_tweet_neg[j] = size\n",
    "    \n",
    "    print(\"counting ended\")\n",
    "    \n",
    "    i=0\n",
    "    pos_mask = np.zeros(np.shape(embeddings)[1]+1)\n",
    "    pos_mask[0] +=1\n",
    "    #adding 1 at start : this is target (1 is for happy emoji, 0 or -1 for sad face)\n",
    "    training_set_pos = np.zeros(((np.shape(pos_train)[0],np.shape(embeddings)[1]+1))) + pos_mask\n",
    "    training_set_neg = np.zeros(((np.shape(neg_train)[0],np.shape(embeddings)[1]+1)))\n",
    "    vocab = open('data/vocab_cut.txt')\n",
    "    #for each word, search if it is in pos_train or neg_train\n",
    "    for j in range(0,np.shape(pos_train)[0]):\n",
    "        list_word = pos_train[j].split()\n",
    "        for i in list_word:\n",
    "            idx = vocab.get(i,-1)\n",
    "            if(idx>=0):\n",
    "                training_set_pos[j,1:np.shape(embeddings)[1]+1] += embeddings[idx]\n",
    "    for j in range(0,np.shape(neg_train)[0]):\n",
    "        list_word = neg_train[j].split()\n",
    "        for i in list_word:\n",
    "            idx = vocab.get(i,-1)\n",
    "            if(idx>=0):\n",
    "                training_set_neg[j,1:np.shape(embeddings)[1]+1] += embeddings[idx]\n",
    "    #then divide by number of words (averaging word vector over all words of the tweet)\n",
    "    np.save('data/trainingset_pos', training_set_pos)\n",
    "    np.save('data/trainingset_neg', training_set_neg)\n",
    "    \n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})\n",
    "\n",
    "def predict_labels(flag=\".npy\"):\n",
    "    #Load the training set\n",
    "    path_neg = str(\"data/trainingset_neg\"+flag)\n",
    "    path_pos = str(\"data/trainingset_pos\"+flag)\n",
    "    ts_neg = np.load(path_neg)\n",
    "    ts_pos = np.load(path_pos)    \n",
    "    #Train a Linear Classifier: Train a linear classifier (e.g. logistic regression or SVM) on your constructed \n",
    "    #features, using the scikit learn library, or your own code from the earlier labs. Recall that the labels \n",
    "    #indicate if a tweet used to contain a :) or :( smiley.\n",
    "    training_set = np.concatenate((ts_neg,ts_pos))\n",
    "    y = training_set[:,0]\n",
    "    X = training_set[:,1:np.shape(training_set)[1]]\n",
    "    #Construct the logistic regressor\n",
    "    LR = sk.LogisticRegression()\n",
    "    #LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, \n",
    "    #class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, \n",
    "    #warm_start=False, n_jobs=1)[source]¶\n",
    "    #http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "    #train the logistic regressor\n",
    "    LR.fit(X,y)\n",
    "\n",
    "    #Now we load and predict the data\n",
    "    data = np.genfromtxt('data/test_data.txt', delimiter=\"\\n\",dtype=str)    \n",
    "    idx = np.zeros(np.shape(data)[0])\n",
    "    tweets = [\"\" for a in range(0,np.shape(data)[0])]\n",
    "    for i in range(0,np.shape(data)[0]):\n",
    "        spliter = data[i].split(\",\")\n",
    "        idx[i] = spliter[0]\n",
    "        tweet = spliter[1]\n",
    "        for j in range(2,np.shape(spliter)[0]):\n",
    "            tweet = tweet+\",\"+spliter[j]\n",
    "        tweets[i] = tweet\n",
    "    #And now, predict the results\n",
    "    topredict = construct_features_for_test_set(tweets)\n",
    "    predictions = LR.predict(topredict)\n",
    "    #Construct the submission\n",
    "    predictions = predictions*2-1\n",
    "    create_csv_submission(idx,predictions,\"submission.csv\")\n",
    "\n",
    "def construct_features_for_test_set(test_set_tweet):\n",
    "    embeddings = np.load('data/embeddings.npy')\n",
    "    \n",
    "    test_set = np.zeros(((np.shape(test_set_tweet)[0],np.shape(embeddings)[1])))\n",
    "    #for each word, search if it is in a tweet\n",
    "    i=0\n",
    "    for word_ in vocab:\n",
    "        word = word_.split(\"\\n\")[0]\n",
    "        if(re.escape(word) != word):\n",
    "            word= re.escape(word)\n",
    "        current_emb = embeddings[i]\n",
    "        for j in range(0,50):\n",
    "            #if yes, add its embeddings.\n",
    "            if re.search(r\"(?:(?:(?:\\b)|^)\"+word+\"(?:(?=\\b)|$)|(?:^|(?:\\s))\"+word+\"(?:$|(?=\\s)))\",test_set_tweet[j]):\n",
    "                count = re.findall(r\"(?:(?:(?:\\b)|^)\"+word+\"(?:(?=\\b)|$)|(?:^|(?:\\s))\"+word+\"(?:$|(?=\\s)))\",test_set_tweet[j])\n",
    "                test_set[j,:] += (len(count)*current_emb)\n",
    "        i+=1\n",
    "    #then divide by number of words (averaging word vector over all words of the tweet)\n",
    "    for i in range(0,np.shape(embeddings)[1]):\n",
    "        test_set[:,i] = test_set[:,i]/word_nbr_per_tweet\n",
    "    return test_set\n",
    "predict_labels()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[-0.26831595 -0.03828992 -0.23159524 -0.11406195 -0.09299987 -0.25714371\n",
      "  0.00553967 -0.13492522 -0.35268678  0.04298145 -0.43047919 -0.13801229\n",
      "  0.72375534  0.03839596 -0.20425634  0.16527752 -0.61413165  0.04258187\n",
      "  0.09463027 -0.10918866]\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.load('data/embeddings.npy')\n",
    "with open('data/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "    vocab_size = len(vocab)\n",
    "print(vocab.get(\"<user>\",-1))\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construct_features():\n",
    "    '''\n",
    "    construct a feature representation of each training tweet \n",
    "    (by averaging the word vectors over all words of the tweet).\n",
    "    '''\n",
    "    #Load the training tweets and the built GloVe word embeddings.\n",
    "    pos_train = open('data/pos_train.txt').readlines()\n",
    "    neg_train = open('data/neg_train.txt').readlines()\n",
    "    embeddings = np.load('data/embeddings.npy')\n",
    "    with open('data/vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    pos_mask = np.zeros(np.shape(embeddings)[1]+1)\n",
    "    pos_mask[0] +=1\n",
    "    #adding 1 at start : this is target (1 is for happy emoji, 0 or -1 for sad face)\n",
    "    training_set_pos = np.zeros(((np.shape(pos_train)[0],np.shape(embeddings)[1]+1))) + pos_mask\n",
    "    training_set_neg = np.zeros(((np.shape(neg_train)[0],np.shape(embeddings)[1]+1)))\n",
    "    #for each word, search if it is in pos_train or neg_train\n",
    "    for j in range(0,np.shape(pos_train)[0]):\n",
    "        list_word = pos_train[j].split()\n",
    "        for i in list_word:\n",
    "            idx = vocab.get(i,-1)\n",
    "            if(idx>=0):\n",
    "                training_set_pos[j,1:np.shape(embeddings)[1]+1] += embeddings[idx]\n",
    "        training_set_pos[j,:] = training_set_pos[j,:]/len(list_word)\n",
    "    for j in range(0,np.shape(neg_train)[0]):\n",
    "        list_word = neg_train[j].split()\n",
    "        for i in list_word:\n",
    "            idx = vocab.get(i,-1)\n",
    "            if(idx>=0):\n",
    "                training_set_neg[j,1:np.shape(embeddings)[1]+1] += embeddings[idx]\n",
    "        training_set_neg[j,:] = training_set_neg[j,:]/len(list_word)\n",
    "    np.save('data/trainingset_pos', training_set_pos)\n",
    "    np.save('data/trainingset_neg', training_set_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construct_features():\n",
    "    '''\n",
    "    construct a feature representation of each training tweet \n",
    "    (by averaging the word vectors over all words of the tweet).\n",
    "    '''\n",
    "    #Load the training tweets and the built GloVe word embeddings.\n",
    "    pos_train = open('data/pos_train.txt').readlines()\n",
    "    neg_train = open('data/neg_train.txt').readlines()\n",
    "    embeddings = np.load('data/embeddings.npy')\n",
    "    with open('data/vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    pos_mask = np.zeros(np.shape(embeddings)[1]+1)\n",
    "    pos_mask[0] +=1\n",
    "    #adding 1 at start : this is target (1 is for happy emoji, 0 or -1 for sad face)\n",
    "    training_set_pos = np.zeros(((np.shape(pos_train)[0],np.shape(embeddings)[1]+1))) + pos_mask\n",
    "    training_set_neg = np.zeros(((np.shape(neg_train)[0],np.shape(embeddings)[1]+1)))\n",
    "    #for each word, search if it is in pos_train or neg_train\n",
    "    for j in range(0,np.shape(pos_train)[0]):\n",
    "        list_word = pos_train[j].split()\n",
    "        for i in list_word:\n",
    "            idx = vocab.get(i,-1)\n",
    "            if(idx>=0):\n",
    "                training_set_pos[j,1:np.shape(embeddings)[1]+1] += embeddings[idx]\n",
    "    for j in range(0,np.shape(neg_train)[0]):\n",
    "        list_word = neg_train[j].split()\n",
    "        for i in list_word:\n",
    "            idx = vocab.get(i,-1)\n",
    "            if(idx>=0):\n",
    "                training_set_neg[j,1:np.shape(embeddings)[1]+1] += embeddings[idx]\n",
    "    np.save('data/trainingset_pos', training_set_pos)\n",
    "    np.save('data/trainingset_neg', training_set_neg)\n",
    "    \n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: ids (event ids associated with each prediction)\n",
    "               y_pred (predicted class labels)\n",
    "               name (string name of .csv output file to be created)\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})\n",
    "\n",
    "def predict_labels(flag=\".npy\"):\n",
    "    #Load the training set\n",
    "    path_neg = str(\"data/trainingset_neg\"+flag)\n",
    "    path_pos = str(\"data/trainingset_pos\"+flag)\n",
    "    ts_neg = np.load(path_neg)\n",
    "    ts_pos = np.load(path_pos)    \n",
    "    #Train a Linear Classifier: Train a linear classifier (e.g. logistic regression or SVM) on your constructed \n",
    "    #features, using the scikit learn library, or your own code from the earlier labs. Recall that the labels \n",
    "    #indicate if a tweet used to contain a :) or :( smiley.\n",
    "    training_set = np.concatenate((ts_neg,ts_pos))\n",
    "    y = training_set[:,0]\n",
    "    X = training_set[:,1:np.shape(training_set)[1]]\n",
    "    #Construct the logistic regressor\n",
    "    LR = sk.LogisticRegression()\n",
    "    #LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, \n",
    "    #class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, \n",
    "    #warm_start=False, n_jobs=1)[source]¶\n",
    "    #http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "    #train the logistic regressor\n",
    "    LR.fit(X,y)\n",
    "\n",
    "    #Now we load and predict the data\n",
    "    data = np.genfromtxt('data/test_data.txt', delimiter=\"\\n\",dtype=str)    \n",
    "    idx = np.zeros(np.shape(data)[0])\n",
    "    tweets = [\"\" for a in range(0,np.shape(data)[0])]\n",
    "    for i in range(0,np.shape(data)[0]):\n",
    "        spliter = data[i].split(\",\")\n",
    "        idx[i] = spliter[0]\n",
    "        tweet = spliter[1]\n",
    "        for j in range(2,np.shape(spliter)[0]):\n",
    "            tweet = tweet+\",\"+spliter[j]\n",
    "        tweets[i] = tweet\n",
    "    #And now, predict the results\n",
    "    topredict = construct_features_for_test_set(tweets)\n",
    "    predictions = LR.predict(topredict)\n",
    "    #Construct the submission\n",
    "    predictions = predictions*2-1\n",
    "    create_csv_submission(idx,predictions,\"submission.csv\")\n",
    "\n",
    "def construct_features_for_test_set(test_set_tweet):\n",
    "    embeddings = np.load('data/embeddings.npy')\n",
    "    with open('data/vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    \n",
    "    test_set = np.zeros((np.shape(test_set_tweet)[0],np.shape(embeddings)[1]))\n",
    "    #for each word, search if it is in a tweet\n",
    "    for j in range(0,np.shape(test_set)[0]):\n",
    "        list_word = test_set_tweet[j].split()\n",
    "        for i in list_word:\n",
    "            idx = vocab.get(i,-1)\n",
    "            if(idx>=0):\n",
    "                test_set[j,:] += embeddings[idx]\n",
    "    #then divide by number of words (averaging word vector over all words of the tweet)\n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
