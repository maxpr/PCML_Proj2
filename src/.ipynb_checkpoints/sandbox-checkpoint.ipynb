{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glove_routines import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "# load in data and labels    \n",
    "#data = np.array(np.loadtxt('data/data.txt'))\n",
    "pos_train = open('data/pos_train.txt').readlines()\n",
    "neg_train = open('data/neg_train.txt').readlines()\n",
    "embeddings = np.load('data/embeddings.npy')\n",
    "vocab = open('data/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running. i is: 0\n",
      "Running. i is: 1000\n",
      "Running. i is: 2000\n",
      "Running. i is: 3000\n",
      "Running. i is: 4000\n",
      "Running. i is: 5000\n",
      "Running. i is: 6000\n",
      "Running. i is: 7000\n",
      "Running. i is: 8000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-838f3729b904>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mtsp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstruct_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/trainingset_pos'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/trainingset_neg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-838f3729b904>\u001b[0m in \u001b[0;36mconstruct_features\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mcurrent_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                 \u001b[0mtraining_set_pos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcurrent_emb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def findWholeWord(w):\n",
    "    return re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search\n",
    "\n",
    "def construct_features():\n",
    "    #Load the training tweets and the built GloVe word embeddings.\n",
    "    \n",
    "    #construct a feature representation of each training tweet \n",
    "    #(by averaging the word vectors over all words of the tweet).\n",
    "    word_nbr_per_tweet_pos = np.zeros(np.shape(pos_train)[0])\n",
    "    for j in range(0,np.shape(pos_train)[0]):\n",
    "        tweet = pos_train[j]\n",
    "        size = len(re.findall(r'\\w+', tweet))\n",
    "        word_nbr_per_tweet_pos[j] = size\n",
    "        \n",
    "    word_nbr_per_tweet_neg = np.zeros(np.shape(neg_train)[0])\n",
    "    for j in range(0,np.shape(neg_train)[0]):\n",
    "        tweet = neg_train[j]\n",
    "        size = len(re.findall(r'\\w+', tweet))\n",
    "        word_nbr_per_tweet_neg[j] = size\n",
    "    \n",
    "    print(\"Counting words in tweet finished.\")\n",
    "    i=0\n",
    "    pos_mask = np.zeros(np.shape(embeddings)[1]+1)\n",
    "    pos_mask[0] +=1\n",
    "    training_set_pos = np.zeros(((np.shape(pos_train)[0],np.shape(embeddings)[1]+1))) + pos_mask\n",
    "    training_set_neg = np.zeros(((np.shape(neg_train)[0],np.shape(embeddings)[1]+1)))\n",
    "    vocab = open('data/vocab_cut.txt')\n",
    "    for word in vocab:\n",
    "        if(i%1000==0):\n",
    "            print(\"Still running.\",i,\"words done.\")\n",
    "        current_emb = embeddings[i]\n",
    "        for j in range(0,np.shape(pos_train)[0]):\n",
    "            if word in pos_train[j]:\n",
    "                training_set_pos[j,1:np.shape(embeddings)[1]+1] += current_emb\n",
    "        for j in range(0,np.shape(neg_train)[0]):\n",
    "            if word in neg_train[j]:\n",
    "                training_set_neg[j,1:np.shape(embeddings)[1]+1] += current_emb\n",
    "        i+=1\n",
    "    print(\"All words done.\")\n",
    "    for i in range(0,20):\n",
    "        training_set_pos[:,i+1] = training_set_pos[:,i+1]/word_nbr_per_tweet_pos\n",
    "        training_set_neg[:,i+1] = training_set_neg[:,i+1]/word_nbr_per_tweet_neg\n",
    "    return training_set_pos,training_set_neg\n",
    "    \n",
    "\n",
    "tsp, tsn = construct_features()\n",
    "np.save('data/trainingset_pos', tsp)\n",
    "np.save('data/trainingset_neg', tsn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 21)\n",
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "word_nbr_per_tweet_pos = np.zeros(np.shape(pos_train)[0])\n",
    "for j in range(0,np.shape(pos_train)[0]):\n",
    "    tweet = pos_train[j]\n",
    "    size = len(re.findall(r'\\w+', tweet))\n",
    "    word_nbr_per_tweet_pos[j] = size\n",
    "\n",
    "training_set_pos = np.zeros(((np.shape(pos_train)[0],np.shape(embeddings)[1]+1)))\n",
    "for i in range(0,20):\n",
    "    training_set_pos[:,i+1] = training_set_pos[:,i+1]/word_nbr_per_tweet_pos\n",
    "print(np.shape(training_set_pos))\n",
    "print(np.shape(training_set_pos[:,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "pos_train = open('data/pos_train.txt').readlines()\n",
    "print(np.shape(pos_train))\n",
    "for i in range(0,np.shape(pos_train)[0]):\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
