{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as sk\n",
    "from glove_routines import *\n",
    "from text_classifier import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216 results found.\n",
      "27 small results found ( 1.5 threshold )\n",
      "Smallest lost for embeddings 5.0 is : 1202747.60121 with params: [5.0, 0.01, 1.0, 300.0, 1.0]\n",
      "   and name: embeddings_cost_emb5_eta0.01_alpha1_nmax300_epochs1.npy\n",
      "Smallest lost for embeddings 30.0 is : 2515917.23658 with params: [30.0, 0.01, 1.0, 300.0, 1.0]\n",
      "   and name: embeddings_cost_emb30_eta0.01_alpha1_nmax300_epochs1.npy\n",
      "Smallest lost for embeddings 50.0 is : 3791732.50916 with params: [50.0, 0.01, 1.0, 300.0, 1.0]\n",
      "   and name: embeddings_cost_emb50_eta0.01_alpha1_nmax300_epochs1.npy\n",
      "Smallest lost for embeddings 100.0 is : 1e+20 with params: [100.0, 0.01, 0.75, 150.0, 4.0]\n",
      "   and name: embeddings_cost_emb100_eta0.01_alpha0.75_nmax150_epochs4.npy\n",
      "Smallest lost for embeddings 100.0 is : 1e+20 with params: [100.0, 0.01, 0.75, 150.0, 4.0]\n",
      "   and name: embeddings_cost_emb100_eta0.01_alpha0.75_nmax150_epochs4.npy\n",
      "    [30.0, 0.01, 0.95, 200.0, 1.0]  loss is  1.49471430216\n",
      "    [30.0, 0.01, 0.95, 250.0, 1.0]  loss is  1.28251491382\n",
      "    [30.0, 0.01, 0.95, 300.0, 1.0]  loss is  1.0913854296\n",
      "    [30.0, 0.01, 0.9, 250.0, 1.0]  loss is  1.40684669869\n",
      "    [30.0, 0.01, 0.9, 300.0, 1.0]  loss is  1.25072507549\n",
      "    [30.0, 0.01, 1.0, 200.0, 1.0]  loss is  1.35901276268\n",
      "    [30.0, 0.01, 1.0, 250.0, 1.0]  loss is  1.1490872137\n",
      "    [30.0, 0.01, 1.0, 300.0, 1.0]  loss is  1.0\n",
      "    [50.0, 0.01, 0.95, 200.0, 1.0]  loss is  1.48425409446\n",
      "    [50.0, 0.01, 0.95, 250.0, 1.0]  loss is  1.24953059041\n",
      "    [50.0, 0.01, 0.95, 300.0, 1.0]  loss is  1.07632517028\n",
      "    [50.0, 0.01, 0.9, 250.0, 1.0]  loss is  1.41860463702\n",
      "    [50.0, 0.01, 0.9, 300.0, 1.0]  loss is  1.23550213098\n",
      "    [50.0, 0.01, 1.0, 200.0, 1.0]  loss is  1.3374044202\n",
      "    [50.0, 0.01, 1.0, 250.0, 1.0]  loss is  1.13761963643\n",
      "    [50.0, 0.01, 1.0, 300.0, 1.0]  loss is  1.0\n",
      "    [5.0, 0.001, 0.9, 200.0, 6.0]  loss is  1.49945567579\n",
      "    [5.0, 0.01, 0.95, 200.0, 1.0]  loss is  1.31728312486\n",
      "    [5.0, 0.01, 0.95, 250.0, 1.0]  loss is  1.19174004817\n",
      "    [5.0, 0.01, 0.95, 300.0, 1.0]  loss is  1.05645838687\n",
      "    [5.0, 0.01, 0.9, 200.0, 1.0]  loss is  1.46097701846\n",
      "    [5.0, 0.01, 0.9, 200.0, 4.0]  loss is  1.39586004457\n",
      "    [5.0, 0.01, 0.9, 250.0, 1.0]  loss is  1.22141299165\n",
      "    [5.0, 0.01, 0.9, 300.0, 1.0]  loss is  1.14889608587\n",
      "    [5.0, 0.01, 1.0, 200.0, 1.0]  loss is  1.27839725368\n",
      "    [5.0, 0.01, 1.0, 250.0, 1.0]  loss is  1.11094273837\n",
      "    [5.0, 0.01, 1.0, 300.0, 1.0]  loss is  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:40: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:41: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:42: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:48: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:62: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def extrac_param(name):\n",
    "    params = []\n",
    "    splited = str(name).split(\"_\")\n",
    "    for i in range(2,np.shape(splited)[0]):\n",
    "        p = splited[i]\n",
    "        value = re.findall(r\"[+-]?\\d+(?:\\.\\d+)?\",p)\n",
    "        params.append(float(value[0]))\n",
    "    return params\n",
    "\n",
    "def choose_parameters(threshold):\n",
    "    '''\n",
    "    Test different parameters to see which one gives the better results.\n",
    "    '''\n",
    "    embedding_dim = [5, 30, 50, 100, 200]\n",
    "    for subdir, dirs, files in os.walk('metadata'):\n",
    "        size = np.shape(files)[0]\n",
    "        losses = np.zeros(size)\n",
    "        names = []\n",
    "        param = []\n",
    "        i=0\n",
    "        for file in files:\n",
    "            f = np.load('metadata/'+str(file))\n",
    "            losses[i] = f\n",
    "            names.append(file)\n",
    "            param.append(extrac_param(file))\n",
    "            i+=1\n",
    "    print(np.shape(names)[0], \"results found.\")\n",
    "    smallest_loss=np.ones(201)*pow(10,20)\n",
    "    smallest_idx = np.zeros(201)\n",
    "    small_lost = []\n",
    "    small_idx = []\n",
    "    small_lost_param = []\n",
    "    for i in range(0,np.shape(losses)[0]):\n",
    "        p = param[i]\n",
    "        emb = p[0]\n",
    "        loss = losses[i]\n",
    "        if(loss<smallest_loss[emb]):\n",
    "            smallest_loss[emb]=loss\n",
    "            smallest_idx[emb] = int(i)\n",
    "            \n",
    "    for i in range(0,np.shape(losses)[0]):\n",
    "        p = param[i]\n",
    "        emb = p[0]\n",
    "        loss = losses[i]\n",
    "        if(loss<threshold*smallest_loss[emb]):\n",
    "            small_lost.append(loss)\n",
    "            small_lost_param.append(p)\n",
    "            small_idx.append(i)\n",
    "\n",
    "    print(np.shape(small_lost)[0], \"small results found (\",threshold,\"threshold )\")\n",
    "\n",
    "    for i in embedding_dim:\n",
    "        cu_params = param[int(smallest_idx[i])]\n",
    "        print(\"Smallest lost for embeddings\",cu_params[0],\"is :\",smallest_loss[i],\"with params:\",cu_params)\n",
    "        print(\"   and name:\",names[int(smallest_idx[i])])\n",
    "    i=0\n",
    "    for a in small_idx:\n",
    "        p = param[a]\n",
    "        print(\"   \",param[a], \" loss is \", small_lost[i]/smallest_loss[p[0]])\n",
    "        i+=1\n",
    "choose_parameters(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_emb100_eta0.01_alpha0.75_nmax150_epochs4.npy\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-509e27a31f05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mconstruct_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"embeddings/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-509e27a31f05>\u001b[0m in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[1;32massert\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m             \u001b[0mconstruct_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"embeddings/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def construct_features(embeddings_file=\"data/embeddings.npy\", flag_save=\"\"):\n",
    "    '''\n",
    "    construct a feature representation of each training tweet \n",
    "    (by averaging the word vectors over all words of the tweet).\n",
    "    '''\n",
    "    #Load the training tweets and the built GloVe word embeddings.\n",
    "    pos_train = open('data/pos_train.txt').readlines()\n",
    "    neg_train = open('data/neg_train.txt').readlines()\n",
    "    embeddings = np.load(embeddings_file)\n",
    "\n",
    "    \n",
    "    #count number of word/tweet and store it\n",
    "    word_nbr_per_tweet_pos = np.zeros(np.shape(pos_train)[0])\n",
    "    for j in range(0,np.shape(pos_train)[0]):\n",
    "        tweet = pos_train[j]\n",
    "        size = len(re.findall(r'\\w+', tweet))\n",
    "        word_nbr_per_tweet_pos[j] = size\n",
    "        \n",
    "    word_nbr_per_tweet_neg = np.zeros(np.shape(neg_train)[0])\n",
    "    for j in range(0,np.shape(neg_train)[0]):\n",
    "        tweet = neg_train[j]\n",
    "        size = len(re.findall(r'\\w+', tweet))\n",
    "        word_nbr_per_tweet_neg[j] = size\n",
    "    \n",
    "    i=0\n",
    "    pos_mask = np.zeros(np.shape(embeddings)[1]+1)\n",
    "    pos_mask[0] +=1\n",
    "    #adding 1 at start : this is target (1 is for happy emoji, 0 or -1 for sad face)\n",
    "    training_set_pos = np.zeros(((np.shape(pos_train)[0],np.shape(embeddings)[1]+1))) + pos_mask\n",
    "    training_set_neg = np.zeros(((np.shape(neg_train)[0],np.shape(embeddings)[1]+1)))\n",
    "    vocab = open('data/vocab_cut.txt')\n",
    "    #for each word, search if it is in pos_train or neg_train\n",
    "    for word_ in vocab:\n",
    "        word = word_.split(\"\\n\")[0]\n",
    "        current_emb = embeddings[i]\n",
    "        for j in range(0,np.shape(pos_train)[0]):\n",
    "            #if yes, add its embeddings.\n",
    "            if word in pos_train[j]:\n",
    "                training_set_pos[j,1:np.shape(embeddings)[1]+1] += current_emb\n",
    "        for j in range(0,np.shape(neg_train)[0]):\n",
    "            if word in neg_train[j]:\n",
    "                training_set_neg[j,1:np.shape(embeddings)[1]+1] += current_emb\n",
    "        i+=1\n",
    "    #then divide by number of words (averaging word vector over all words of the tweet)\n",
    "    for i in range(0,np.shape(embeddings)[1]):\n",
    "        training_set_pos[:,i+1] = training_set_pos[:,i+1]/word_nbr_per_tweet_pos\n",
    "        training_set_neg[:,i+1] = training_set_neg[:,i+1]/word_nbr_per_tweet_neg\n",
    "    np.save(str('data/trainingset_pos')+flag_save, training_set_pos)\n",
    "    np.save(str('data/trainingset_neg')+flag_save, training_set_neg)\n",
    "    \n",
    "def test():\n",
    "     for subdir, dirs, files in os.walk('embeddings'):\n",
    "        for file in files:\n",
    "            print(file)\n",
    "            assert 0==1\n",
    "            construct_features(\"embeddings/\"+file)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216 results found.\n",
      "(216,)\n",
      "(216, 5)\n",
      "(216,)\n"
     ]
    }
   ],
   "source": [
    "def fix_param(data,idx,value):\n",
    "    if(data[idx].any!=value):\n",
    "        print(\"FAIL\")\n",
    "\n",
    "def show_graph():\n",
    "    embedding_dim = [5, 30, 50, 100, 200]\n",
    "    for subdir, dirs, files in os.walk('metadata'):\n",
    "        size = np.shape(files)[0]\n",
    "        losses = np.zeros(size)\n",
    "        names = []\n",
    "        param = []\n",
    "        i=0\n",
    "        for file in files:\n",
    "            f = np.load('metadata/'+str(file))\n",
    "            losses[i] = f\n",
    "            names.append(file)\n",
    "            param.append(extrac_param(file))\n",
    "            i+=1\n",
    "    print(np.shape(names)[0], \"results found.\")\n",
    "    print(np.shape(names))\n",
    "    print(np.shape(param))\n",
    "    print(np.shape(losses))\n",
    "show_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
