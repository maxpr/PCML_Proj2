{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as sk\n",
    "from glove_routines import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "ts_neg = np.load('data/trainingset_neg.npy')\n",
    "ts_pos = np.load('data/trainingset_pos.npy')\n",
    "\n",
    "#y = np.genfromtxt('data/test_data.txt', delimiter=\",\", skip_header=1, dtype=str, usecols=0)\n",
    "x = np.genfromtxt('data/test_data.txt', delimiter=\"\\n\",dtype=str)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "def predict_labels(data,flag=\"\"):\n",
    "    data = np.genfromtxt('data/test_data.txt', delimiter=\"\\n\",dtype=str)    \n",
    "    idx = np.zeros(np.shape(data)[0])\n",
    "    tweets = [\"\" for a in range(0,np.shape(data)[0])]\n",
    "    for i in range(0,np.shape(data)[0]):\n",
    "        spliter = data[i].split(\",\")\n",
    "        idx[i] = spliter[0]\n",
    "        tweet = spliter[1]\n",
    "        for j in range(2,np.shape(spliter)[0]):\n",
    "            tweet = tweet+\",\"+spliter[j]\n",
    "        tweets[i] = tweet\n",
    "    \n",
    "    \n",
    "    path_neg = str(\"data/trainingset_neg_\"+flag)\n",
    "    path_pos = str(\"data/trainingset_pos_\"+flag)\n",
    "    ts_neg = np.load(path_neg)\n",
    "    ts_pos = np.load(path_pos)    \n",
    "    #Train a Linear Classifier: Train a linear classifier (e.g. logistic regression or SVM) on your constructed \n",
    "    #features, using the scikit learn library, or your own code from the earlier labs. Recall that the labels \n",
    "    #indicate if a tweet used to contain a :) or :( smiley.\n",
    "    training_set = np.concatenate((ts_neg,ts_pos))\n",
    "    y = training_set[:,0]\n",
    "    X = training_set[:,1:np.shape(training_set)[1]]\n",
    "    LR = sk.LogisticRegression()\n",
    "    #LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, \n",
    "    #class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, \n",
    "    #warm_start=False, n_jobs=1)[source]¶\n",
    "    #http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "    LR.fit(X,y)\n",
    "    predictions = LR.predict(tweets)\n",
    "    submission = np.concatenate(idx,submission)\n",
    "    np.save('data/submission.csv', submission)\n",
    "    return LR.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 10000 features per sample; expecting 20",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-8963db25f760>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-f4008b62ccad>\u001b[0m in \u001b[0;36mpredict_labels\u001b[1;34m(data, flag)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m#http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mLR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0msubmission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msubmission\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/submission.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubmission\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andre\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \"\"\"\n\u001b[1;32m--> 336\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andre\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 317\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 10000 features per sample; expecting 20"
     ]
    }
   ],
   "source": [
    "predict_labels(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_labels(data,flag=\".npy\"):\n",
    "    #Load the training set\n",
    "    path_neg = str(\"data/trainingset_neg\"+flag)\n",
    "    path_pos = str(\"data/trainingset_pos\"+flag)\n",
    "    ts_neg = np.load(path_neg)\n",
    "    ts_pos = np.load(path_pos)    \n",
    "    #Train a Linear Classifier: Train a linear classifier (e.g. logistic regression or SVM) on your constructed \n",
    "    #features, using the scikit learn library, or your own code from the earlier labs. Recall that the labels \n",
    "    #indicate if a tweet used to contain a :) or :( smiley.\n",
    "    training_set = np.concatenate((ts_neg,ts_pos))\n",
    "    y = training_set[:,0]\n",
    "    X = training_set[:,1:np.shape(training_set)[1]]\n",
    "    #Construct the logistic regressor\n",
    "    LR = sk.LogisticRegression()\n",
    "    #LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, \n",
    "    #class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, \n",
    "    #warm_start=False, n_jobs=1)[source]¶\n",
    "    #http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "    #train the logistic regressor\n",
    "    LR.fit(X,y)\n",
    "    \n",
    "    #Now we load and predict the data\n",
    "    data = np.genfromtxt('data/test_data.txt', delimiter=\"\\n\",dtype=str)    \n",
    "    idx = np.zeros(np.shape(data)[0])\n",
    "    tweets = [\"\" for a in range(0,np.shape(data)[0])]\n",
    "    for i in range(0,np.shape(data)[0]):\n",
    "        spliter = data[i].split(\",\")\n",
    "        idx[i] = spliter[0]\n",
    "        tweet = spliter[1]\n",
    "        for j in range(2,np.shape(spliter)[0]):\n",
    "            tweet = tweet+\",\"+spliter[j]\n",
    "        tweets[i] = tweet\n",
    "    #And now, predict the results\n",
    "    predictions = LR.predict(construct_features_for_test_set(tweets))\n",
    "    #Construct the submission\n",
    "    submission = np.concatenate(idx,submission)\n",
    "    np.save('data/submission.csv', submission)\n",
    "\n",
    "def construct_features_for_test_set(test_set_tweet):\n",
    "    embeddings = np.load('data/embeddings.npy')\n",
    "    word_nbr_per_tweet = np.zeros(np.shape(test_set_tweet)[0])\n",
    "    \n",
    "    for j in range(0,np.shape(test_set_tweet)[0]):\n",
    "        tweet = test_set_tweet[j]\n",
    "        size = len(re.findall(r'\\w+', tweet))\n",
    "        word_nbr_per_tweet[j] = size\n",
    "    \n",
    "    vocab = open('data/vocab_cut.txt')\n",
    "    test_set = np.zeros(((np.shape(test_set_tweet)[0],np.shape(embeddings)[1])))\n",
    "    #for each word, search if it is in a tweet\n",
    "    a=0\n",
    "    i=0\n",
    "    print(np.shape(test_set))\n",
    "    print(np.shape(test_set_tweet))\n",
    "    for word in vocab:\n",
    "        current_emb = embeddings[i]\n",
    "        for j in range(0,50):\n",
    "            #if yes, add its embeddings.\n",
    "            if word in test_set_tweet[j]:\n",
    "                a=j\n",
    "                test_set[j,:] += current_emb\n",
    "        i+=1\n",
    "    #then divide by number of words (averaging word vector over all words of the tweet)\n",
    "    for i in range(0,np.shape(embeddings)[1]):\n",
    "        test_set[:,i] = test_set[:,i]/word_nbr_per_tweet\n",
    "    return test_set,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20)\n",
      "(10000,)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "vocab = open('data/vocab_cut.txt')\n",
    "for word in vocab:\n",
    "        current_emb = embeddings[i]\n",
    "        for j in range(0,50):\n",
    "            #if yes, add its embeddings.\n",
    "            if word in test_set_tweet[j]:\n",
    "                a=j\n",
    "                test_set[j,:] += current_emb\n",
    "        i+=1\n",
    "        if a!=0:\n",
    "            break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,<user> no ma'am ! ! ! lol im perfectly fine and not contagious anymore lmao\n",
      "(10000,)\n",
      "on the west\n",
      "<user> <user> <user> <user> lol don't worry , kristopher we making another witchu\n",
      "44.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "data = np.genfromtxt('data/test_data.txt', delimiter=\"\\n\",dtype=str)    \n",
    "print(data[3])\n",
    "idx = np.zeros(np.shape(data)[0])\n",
    "tweets = [\"\" for a in range(0,np.shape(data)[0])]\n",
    "for i in range(0,np.shape(data)[0]):\n",
    "    spliter = data[i].split(\",\")\n",
    "    idx[i] = spliter[0]\n",
    "    tweet = spliter[1]\n",
    "    for j in range(2,np.shape(spliter)[0]):\n",
    "        tweet = tweet+\",\"+spliter[j]\n",
    "    tweets[i] = tweet\n",
    "\n",
    "#teez = construct_features_for_test_set(tweets)\n",
    "print(np.shape(tweets))\n",
    "print(tweets[434])\n",
    "print(tweets[232])\n",
    "print(idx[43])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "zaefsfd[2]\n",
    "print(ef)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
