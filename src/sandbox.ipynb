{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glove_routines import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "# load in data and labels    \n",
    "#data = np.array(np.loadtxt('data/data.txt'))\n",
    "pos_train = open('data/pos_train.txt').readlines()\n",
    "neg_train = open('data/neg_train.txt').readlines()\n",
    "embeddings = np.load('data/embeddings.npy')\n",
    "vocab = open('data/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting words in tweet finished.\n",
      "Still running. 0 words done.\n",
      "Still running. 1000 words done.\n",
      "Still running. 2000 words done.\n",
      "Still running. 3000 words done.\n",
      "Still running. 4000 words done.\n",
      "Still running. 5000 words done.\n",
      "Still running. 6000 words done.\n",
      "Still running. 7000 words done.\n",
      "Still running. 8000 words done.\n",
      "Still running. 9000 words done.\n",
      "Still running. 10000 words done.\n",
      "Still running. 11000 words done.\n",
      "Still running. 12000 words done.\n",
      "Still running. 13000 words done.\n",
      "Still running. 14000 words done.\n",
      "Still running. 15000 words done.\n",
      "Still running. 16000 words done.\n",
      "Still running. 17000 words done.\n",
      "Still running. 18000 words done.\n",
      "Still running. 19000 words done.\n",
      "Still running. 20000 words done.\n",
      "Still running. 21000 words done.\n",
      "All words done.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def findWholeWord(w):\n",
    "    return re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search\n",
    "\n",
    "def construct_features():\n",
    "    #Load the training tweets and the built GloVe word embeddings.\n",
    "    \n",
    "    #construct a feature representation of each training tweet \n",
    "    #(by averaging the word vectors over all words of the tweet).\n",
    "    word_nbr_per_tweet_pos = np.zeros(np.shape(pos_train)[0])\n",
    "    for j in range(0,np.shape(pos_train)[0]):\n",
    "        tweet = pos_train[j]\n",
    "        size = len(re.findall(r'\\w+', tweet))\n",
    "        word_nbr_per_tweet_pos[j] = size\n",
    "        \n",
    "    word_nbr_per_tweet_neg = np.zeros(np.shape(neg_train)[0])\n",
    "    for j in range(0,np.shape(neg_train)[0]):\n",
    "        tweet = neg_train[j]\n",
    "        size = len(re.findall(r'\\w+', tweet))\n",
    "        word_nbr_per_tweet_neg[j] = size\n",
    "    \n",
    "    print(\"Counting words in tweet finished.\")\n",
    "    i=0\n",
    "    pos_mask = np.zeros(np.shape(embeddings)[1]+1)\n",
    "    pos_mask[0] +=1\n",
    "    training_set_pos = np.zeros(((np.shape(pos_train)[0],np.shape(embeddings)[1]+1))) + pos_mask\n",
    "    training_set_neg = np.zeros(((np.shape(neg_train)[0],np.shape(embeddings)[1]+1)))\n",
    "    vocab = open('data/vocab_cut.txt')\n",
    "    for word in vocab:\n",
    "        if(i%1000==0):\n",
    "            print(\"Still running.\",i,\"words done.\")\n",
    "        current_emb = embeddings[i]\n",
    "        for j in range(0,np.shape(pos_train)[0]):\n",
    "            if word in pos_train[j]:\n",
    "                training_set_pos[j,1:np.shape(embeddings)[1]+1] += current_emb\n",
    "        for j in range(0,np.shape(neg_train)[0]):\n",
    "            if word in neg_train[j]:\n",
    "                training_set_neg[j,1:np.shape(embeddings)[1]+1] += current_emb\n",
    "        i+=1\n",
    "    print(\"All words done.\")\n",
    "    for i in range(0,20):\n",
    "        training_set_pos[:,i+1] = training_set_pos[:,i+1]/word_nbr_per_tweet_pos\n",
    "        training_set_neg[:,i+1] = training_set_neg[:,i+1]/word_nbr_per_tweet_neg\n",
    "    return training_set_pos,training_set_neg\n",
    "    \n",
    "\n",
    "tsp, tsn = construct_features()\n",
    "np.save('data/trainingset_pos', tsp)\n",
    "np.save('data/trainingset_neg', tsn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,  -2.84709695e-02,   9.83739275e-02,\n",
       "         -9.81503952e-02,  -6.77745421e-02,  -4.21848768e-02,\n",
       "         -1.35403593e-01,  -3.98721924e-02,  -1.54661829e-01,\n",
       "         -1.05290432e-01,   1.26775989e-01,  -1.07313959e-01,\n",
       "         -1.93376610e-01,  -2.43938538e-02,   3.28113079e-02,\n",
       "          4.24094316e-02,  -1.01534101e-01,  -1.33482254e-01,\n",
       "          3.61616554e-02,   4.77668273e-04,  -2.04354702e-02]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsp[10324:10325]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "pos_train = open('data/pos_train.txt').readlines()\n",
    "print(np.shape(pos_train))\n",
    "for i in range(0,np.shape(pos_train)[0]):\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
